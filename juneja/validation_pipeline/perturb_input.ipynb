{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b03623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp perturb_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab13d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7a776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 14:36:24.883163: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-22 14:36:34.790557: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.3/lib64:/scratch/juneja/python/lib:/usr/local/cuda-11.3/tensorRT-8.0/lib64:/usr/local/cuda-11.3/cudnn-8.2/lib64:/usr/local/cuda-11.3/nccl-2.9/lib64\n",
      "2023-10-22 14:36:34.790908: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.3/lib64:/scratch/juneja/python/lib:/usr/local/cuda-11.3/tensorRT-8.0/lib64:/usr/local/cuda-11.3/cudnn-8.2/lib64:/usr/local/cuda-11.3/nccl-2.9/lib64\n",
      "2023-10-22 14:36:34.790939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# def apply_grey_patch(path, image, top_left_x, top_left_y, patch_size):\n",
    "\n",
    "\n",
    "def apply_grey_patch(image, top_left_x, top_left_y, patch_size):\n",
    "    patched_image = np.array(image, copy=True)\n",
    "    patched_image[top_left_y:top_left_y + patch_size,\n",
    "                  top_left_x:top_left_x + patch_size, :] = 0\n",
    "#     img = keras.preprocessing.image.array_to_img(patched_image)\n",
    "#     print(path)\n",
    "#     img.save(path)\n",
    "    return patched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e961e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "# import preprocess\n",
    "\n",
    "#have a version with multiple occlusion blocks\n",
    "# def occu_perturb(img_path, out_path, gt_saliency_map):\n",
    "def occu_perturb(img_arr, gt_saliency_map):\n",
    "  \n",
    "    \n",
    "#     image = preprocess.img_to_tensor(img_path)\n",
    "\n",
    "#     image = keras.preprocessing.image.load_img(img_path, target_size=(300, 300))\n",
    "#     image = keras.preprocessing.image.img_to_array(image)\n",
    "    \n",
    "#     i = 0\n",
    "\n",
    "    PATCH_SIZE = 60\n",
    "    occluded_img_list = []\n",
    "    for top_left_x in range(0, img_arr.shape[0], PATCH_SIZE):\n",
    "        for top_left_y in range(0, img_arr.shape[1], PATCH_SIZE):\n",
    "            patched_image = apply_grey_patch(img_arr, top_left_x, top_left_y, PATCH_SIZE)\n",
    "            patched_image = patched_image.astype('float32') / 255.0\n",
    "            occluded_img_list.append(patched_image)\n",
    "            \n",
    "            # Apply the patch and display the image\n",
    "#             path = out_path+'/'+ 'occluded_img_'+str(i)+'.jpg'\n",
    "            \n",
    "#           patched_image = apply_grey_patch(path,image, top_left_x, top_left_y, PATCH_SIZE)\n",
    "            \n",
    "            \n",
    "#             i+=1\n",
    "        \n",
    "    return occluded_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c061741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an empty list to store the perturbed images\n",
    "# perturbed_images_list = []\n",
    "\n",
    "# # Iterate through each original image, limited to the first 19 images\n",
    "# for i in range(1, 20):\n",
    "#     original_image = images[i]\n",
    "#     # Initialize an empty list to store perturbed sub-images for this original image\n",
    "#     perturbed_sub_images = []\n",
    "\n",
    "#     # Iterate through the sub-grid, applying patches\n",
    "#     for top_left_x in range(0, original_image.shape[0], PATCH_SIZE):\n",
    "#         for top_left_y in range(0, original_image.shape[1], PATCH_SIZE):\n",
    "#             # Apply the grey patch to the original image\n",
    "#             perturbed_image = np.array(original_image, copy=True)\n",
    "#             perturbed_image[top_left_y:top_left_y + PATCH_SIZE, top_left_x:top_left_x + PATCH_SIZE, :] = 0\n",
    "#             perturbed_image = perturbed_image.astype('float32') / 255.0\n",
    "\n",
    "#             # Append the perturbed sub-image to the list\n",
    "#             perturbed_sub_images.append(perturbed_image)\n",
    "    \n",
    "#     # Append the list of perturbed sub-images to the perturbed images list for this original image\n",
    "#     perturbed_images_list.append(perturbed_sub_images)\n",
    "\n",
    "# # Convert the list of perturbed images to a NumPy array\n",
    "# perturbed_images_array = np.array(perturbed_images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28700bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def noise_perturb_image(img_tensor, saliency_map, perturbation_strength=0.05, saliency_threshold=0.5, num_iterations=1, perturbation_prob=0.1):\n",
    "    \n",
    "    image = img_tensor.to(saliency_map.device)\n",
    "    saliency_values = saliency_map.view(-1)\n",
    "    threshold = torch.kthvalue(saliency_values, int((1 - saliency_threshold) * saliency_values.size(0)) + 1).values\n",
    "    mask = torch.gt(saliency_map, threshold).float()\n",
    "    \n",
    "    # Perturb the image for a given number of iterations only at a random subset of the mask where the value is 1\n",
    "    perturbed_image = image.clone()\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        perturbation_mask = (mask == 1) * (torch.rand_like(mask) < perturbation_prob).float()\n",
    "        noise = torch.randn_like(perturbed_image) * perturbation_strength\n",
    "        perturbation_mask = perturbation_mask.to(perturbed_image.device)  # Move perturbation_mask to the same device as perturbed_image\n",
    "        noise = noise.to(perturbed_image.device)  # Move noise to the same device as perturbed_image\n",
    "        perturbed_image = perturbed_image + perturbation_mask * noise\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e405a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from preprocess import img_to_tensor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from run_pipe import inference\n",
    "\n",
    "def noise_perturb(img_path, out_dir, gt_saliency_map):\n",
    "    img_tensor = img_to_tensor(img_path)\n",
    "    saliency_threshold = 0.5\n",
    "    perturbation_strength = 0.1\n",
    "    perturbation_prob = 0.1\n",
    "    for i in range(10):\n",
    "        perturbed_tensor = noise_perturb_image(img_tensor, gt_saliency_map, perturbation_strength, saliency_threshold, 1, perturbation_prob)\n",
    "        image_array = perturbed_tensor.squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "        image_array = (image_array - np.min(image_array)) / (np.max(image_array) - np.min(image_array))    \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image_array, cmap = 'turbo', alpha = 0.8)\n",
    "        plt.axis('off')\n",
    "        fig.savefig(f\"{out_dir}/{i}_noise.jpeg\")\n",
    "\n",
    "        perturbation_strength -= 0.01\n",
    "        saliency_threshold += 0.005\n",
    "        \n",
    "#         inference(perturbed_tensor, model, out_dir, sal_method)\n",
    "\n",
    "#         perturbed_tensor = perturbed_tensor.type_as(gpu_reference_tensor)\n",
    "#         saliency_map_perturbed, perturbed_idx = wrapped_model(perturbed_tensor)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             output = model(perturbed_tensor)\n",
    "#             perturbed_class = torch.argmax(output).item()\n",
    "#             prob = torch.softmax(output, dim=1)[0, perturbed_class].item()\n",
    "#             img = reverse_normalize(perturbed_tensor)\n",
    "#             heatmap = visualize(img, saliency_map_perturbed)\n",
    "#             hm = (heatmap.squeeze().numpy().transpose(1, 2, 0))\n",
    "            \n",
    "#             ax.imshow(hm, cmap='turbo', alpha = 0.8)\n",
    "#             ax.set_title(f\"Class: {classes[perturbed_idx]}({prob*100:.2f}%)\")\n",
    "#             fig.savefig(f\"{out_dir}/saliency/after_noise/{i}_{idx2label[idx].replace(' ','-')}.jpeg\")\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ca55f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "from nbdev.export import nb_export\n",
    "nb_export('perturb_input.ipynb', './')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
