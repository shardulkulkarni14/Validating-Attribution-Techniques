{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af233ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for input\n",
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f026b29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nchoices for the techniques that would be run\\n\\n1. model selection : CNN based, Attention based, both\\n2. input perturbation test: occlusion, noise\\n\\nFinal results: Metrics: significance and what they mean\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UI here\n",
    "\n",
    "'''\n",
    "choices for the techniques that would be run\n",
    "\n",
    "1. model selection : CNN based, Attention based, both\n",
    "2. input perturbation test: occlusion, noise\n",
    "\n",
    "Final results: Metrics: significance and what they mean\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a8c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 10 19:38:15 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     Off | 00000000:B1:00.0 Off |                  N/A |\r\n",
      "| 22%   24C    P8               7W / 250W |    725MiB / 11264MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce RTX 2080 Ti     Off | 00000000:B2:00.0 Off |                  N/A |\r\n",
      "| 22%   23C    P8               1W / 250W |      1MiB / 11264MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA GeForce RTX 2080 Ti     Off | 00000000:DA:00.0 Off |                  N/A |\r\n",
      "| 22%   24C    P8              17W / 250W |      1MiB / 11264MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA GeForce RTX 2080 Ti     Off | 00000000:DB:00.0 Off |                  N/A |\r\n",
      "| 22%   24C    P8              21W / 250W |      1MiB / 11264MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1402b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verification in captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d7a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradcam with saliency maps in pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b010637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abid: add ui code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad80404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInput\\n1. image \\n2. perturbation technique (occlusion, yada yada) \\n3. introspection method\\n4. resulting heatmaps\\n5. metrics on display\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced24fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 19:38:24.111044: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-10 19:38:31.846246: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /scratch/juneja/python/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda-11.3/lib64:/scratch/juneja/python/lib:/usr/local/cuda-11.3/tensorRT-8.0/lib64:/usr/local/cuda-11.3/cudnn-8.2/lib64:/usr/local/cuda-11.3/nccl-2.9/lib64\n",
      "2023-10-10 19:38:31.846603: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /scratch/juneja/python/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda-11.3/lib64:/scratch/juneja/python/lib:/usr/local/cuda-11.3/tensorRT-8.0/lib64:/usr/local/cuda-11.3/cudnn-8.2/lib64:/usr/local/cuda-11.3/nccl-2.9/lib64\n",
      "2023-10-10 19:38:31.846637: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import scipy.stats as sci "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa257ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a440c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set proxy\n",
    "\n",
    "os.environ['HTTP_PROXY'] = 'http://fp.cs.ovgu.de:3210/'\n",
    "os.environ['HTTPS_PROXY'] = 'http://fp.cs.ovgu.de:3210/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72ac59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import skimage.transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from ScoreCAM import cam\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from cam import CAM, GradCAM, GradCAMpp, SmoothGradCAMpp, ScoreCAM\n",
    "\n",
    "from utils.visualize import visualize, reverse_normalize\n",
    "from utils.imagenet_labels import label2idx, idx2label\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from pyemd import emd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7643ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guy working on data science: woot woot!\n",
    "# okay\n",
    "\n",
    "#choose an image path\n",
    "path = ''\n",
    "\n",
    "#checkbox methods you want to run\n",
    "methods = \"GRADCAM, SCORECAM, LAYERCAM \"\n",
    "\n",
    "#let it run all perturbation methods\n",
    "#occulusion, noising, etc\n",
    "\n",
    "#report the metrics in a table\n",
    "#Descirbe what the metrics do and report them in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "077102e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nbdev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57d00a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36e3f7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jupyterlab-quarto in /scratch/juneja/python/lib/python3.8/site-packages (0.2.8)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jupyterlab-quarto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e9609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c1dfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nStep1: make user upload an image via image upload wizard\\n\\nStep2: make transformations to the image in the background\\n\\nStep3: run techniques selected sequentially\\n#checkboxes : gradcam, scorecam, whatever\\n\\nfor technique \\nif(technique === 'gradcam','layercam','scorecam')\\ngradcam = gradcam(image)\\n\\nStep4: make user select perturbation techniques:\\n1. Import perturbation_methods\\n2. make the image go through perturbation methods\\n3. run the explaination techniques on the perturbed images.\\n\\n#checkboxes : occlusion, noise levels\\n\\nStep 5: give out meaningful metrics\\nimport metrics.py\\nmetrics(saliency_map1, saliency_map2)\\n\\n#table\\n\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Step1: make user upload an image via image upload wizard\n",
    "\n",
    "Step2: make transformations to the image in the background\n",
    "\n",
    "Step3: run techniques selected sequentially\n",
    "#checkboxes : gradcam, scorecam, whatever\n",
    "\n",
    "for technique \n",
    "if(technique === 'gradcam','layercam','scorecam')\n",
    "gradcam = gradcam(image)\n",
    "\n",
    "Step4: make user select perturbation techniques:\n",
    "1. Import perturbation_methods\n",
    "2. make the image go through perturbation methods\n",
    "3. run the explaination techniques on the perturbed images.\n",
    "\n",
    "#checkboxes : occlusion, noise levels\n",
    "\n",
    "Step 5: give out meaningful metrics\n",
    "import metrics.py\n",
    "metrics(saliency_map1, saliency_map2)\n",
    "\n",
    "#table\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9d83a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f136c850686b4cd09a5f2040ec68e66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='image/*', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets.widgets import FileUpload, Label, FloatProgress, FloatSlider, Button, RadioButtons, Checkbox, Dropdown, HTML, Image, ToggleButtons\n",
    "from ipywidgets.widgets import Layout, HBox, VBox\n",
    "from IPython.display import display\n",
    "# import bqplot as bq\n",
    "# import time\n",
    "# import threading\n",
    "\n",
    "FileUpload(\n",
    "    accept='image/*',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
    "    multiple=False  # True to accept multiple files upload else False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3f3bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "177b00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Preprocessing\n",
    "    def preprocessing(image):\n",
    "        size = (300, 300)\n",
    "        image = image.resize(size)\n",
    "        #mean and std from ImageNet\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "           std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        tensor = preprocess(image)\n",
    "        # reshape 4D tensor (N, C, H, W)\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5f0b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = preprocessing(image)\n",
    "# tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cab9e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/juneja/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/scratch/juneja/python/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/juneja/python/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de7645ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_reference_tensor = next(model.parameters())\n",
    "# tensor = tensor.type_as(gpu_reference_tensor)\n",
    "# target_layer = model.layer4[1].conv2\n",
    "# wrapped_model = ScoreCAM(model, target_layer)\n",
    "# cam, idx = wrapped_model(tensor)\n",
    "# classes = open(\"/project/validating_attribution_techniques/shardul/classes.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38d347be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# print(idx)\n",
    "# img = reverse_normalize(tensor)\n",
    "# heatmap = visualize(img, cam)\n",
    "# heatmap = np.transpose(heatmap.squeeze(), (1, 2, 0))\n",
    "# ax.imshow(heatmap, cmap='turbo', alpha = 0.8)\n",
    "# ax.set_title(f\"Class: {classes[idx]}\")\n",
    "# fig.savefig(f\"/project/validating_attribution_techniques/shardul/output/saliency/before_noise/{idx2label[idx]}.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a66fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.export import nb_export\n",
    "nb_export('saliency_verification_tests.ipynb') # arg1 : name of notebook, arg2 : location of the export relative to the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4987fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp perturbation_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ccc897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#Creating the patch\n",
    "PATCH_SIZE = 60\n",
    "def apply_grey_patch(path, image, top_left_x, top_left_y, patch_size):\n",
    "    patched_image = np.array(image, copy=True)\n",
    "    patched_image[top_left_y:top_left_y + patch_size, top_left_x:top_left_x + patch_size, :] = 0\n",
    "    img = keras.preprocessing.image.array_to_img(patched_image)\n",
    "    print(path)\n",
    "    img.save(path)\n",
    "    print(np.shape(patched_image))\n",
    "    return patched_image\n",
    "\n",
    "def add_random_noise_to_image(image, saliency_map, perturbation_strength=0.05, saliency_threshold=0.5, num_iterations=1, perturbation_prob=0.1):\n",
    "    \"\"\"perturbation_strength: This parameter controls the strength of the perturbation applied to the image. \n",
    "    It determines the magnitude of the noise added to the image.\n",
    "    \n",
    "    saliency_threshold: This parameter determines the percentage of saliency values to consider as the top values.\n",
    "    It is used to threshold the saliency map and generate a binary mask.\n",
    "    \n",
    "    num_iterations: This parameter specifies the number of iterations to perturb the image. In each iteration, \n",
    "    the perturbation is applied to the image based on the saliency mask.\n",
    "    \n",
    "    perturbation_prob: This parameter controls the probability of perturbing each pixel in the saliency mask.\n",
    "    It determines the randomness of the perturbation.\"\"\"\n",
    "    \n",
    "    # Move the image tensor to the device of the saliency_map tensor\n",
    "    image = image.to(saliency_map.device)\n",
    "    \n",
    "    # Get the top n percent of saliency values\n",
    "    saliency_values = saliency_map.view(-1)\n",
    "    threshold = torch.kthvalue(saliency_values, int((1 - saliency_threshold) * saliency_values.size(0)) + 1).values\n",
    "    \n",
    "    # Threshold the saliency map and generate a mask\n",
    "    mask = torch.gt(saliency_map, threshold).float()\n",
    "    \n",
    "    # Perturb the image for a given number of iterations only at a random subset of the mask where the value is 1\n",
    "    perturbed_image = image.clone()\n",
    "    for i in range(num_iterations):\n",
    "        perturbation_mask = (mask == 1) * (torch.rand_like(mask) < perturbation_prob).float()\n",
    "        noise = torch.randn_like(perturbed_image) * perturbation_strength\n",
    "        perturbation_mask = perturbation_mask.to(perturbed_image.device)  # Move perturbation_mask to the same device as perturbed_image\n",
    "        noise = noise.to(perturbed_image.device)  # Move noise to the same device as perturbed_image\n",
    "        perturbed_image = perturbed_image + perturbation_mask * noise\n",
    "        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "        \n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4b33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b48a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.export import nb_export\n",
    "nb_export('saliency_verification_tests.ipynb','./') # arg1 : name of notebook, arg2 : location of the export relative to the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aec03e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3c3266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "from skimage import exposure\n",
    "\n",
    "def normalize(x, method='standard', axis=None):\n",
    "    '''Normalizes the input with specified method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : numpy.ndarray\n",
    "        Normalized array.\n",
    "    '''\n",
    "    # TODO: Prevent divided by zero if the map is flat\n",
    "    x = np.array(x, copy=False)\n",
    "    if axis is not None:\n",
    "        y = np.rollaxis(x, axis).reshape([x.shape[axis], -1])\n",
    "        shape = np.ones(len(x.shape))\n",
    "        shape[axis] = x.shape[axis]\n",
    "        if method == 'standard':\n",
    "            res = (x - np.mean(y, axis=1).reshape(shape)) / np.std(y, axis=1).reshape(shape)\n",
    "        elif method == 'range':\n",
    "            res = (x - np.min(y, axis=1).reshape(shape)) / (np.max(y, axis=1) - np.min(y, axis=1)).reshape(shape)\n",
    "        elif method == 'sum':\n",
    "            res = x / np.float_(np.sum(y, axis=1).reshape(shape))\n",
    "        else:\n",
    "            raise ValueError('method not in {\"standard\", \"range\", \"sum\"}')\n",
    "    else:\n",
    "        if method == 'standard':\n",
    "            res = (x - np.mean(x)) / np.std(x)\n",
    "        elif method == 'range':\n",
    "            res = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "        elif method == 'sum':\n",
    "            res = x / float(np.sum(x))\n",
    "        else:\n",
    "            raise ValueError('method not in {\"standard\", \"range\", \"sum\"}')\n",
    "    return res\n",
    "\n",
    "\n",
    "def match_hist(image, cdf, bin_centers, nbins=256):\n",
    "    '''\n",
    "    References\n",
    "    ----------\n",
    "    [1] Matlab implementation histoMatch(MTX, N, X) by Simoncelli, 7/96.\n",
    "    '''\n",
    "    image = img_as_float(image)\n",
    "    old_cdf, old_bin = exposure.cumulative_distribution(image, nbins) # Unlike [1], we didn't add small positive number to the histogram\n",
    "    new_bin = np.interp(old_cdf, cdf, bin_centers)\n",
    "    out = np.interp(image.ravel(), old_bin, new_bin)\n",
    "    return out.reshape(image.shape)\n",
    "\n",
    "def normalize_map(map):\n",
    "    min_value = np.min(map)\n",
    "    max_value = np.max(map)\n",
    "    normalized_map = (map - min_value) / (max_value - min_value)\n",
    "    return normalized_map\n",
    "\n",
    "def SIM(saliency_map1, saliency_map2):\n",
    "    ''' \n",
    "    saliency maps when viewed as distributions, histogram intersection\n",
    "    (SIM=1 means the distributions are identical).\n",
    "    '''\n",
    "    map1 = np.array(saliency_map1, copy=False)\n",
    "    map2 = np.array(saliency_map2, copy=False)\n",
    "    if map1.shape != map2.shape:\n",
    "        map1 = resize(map1, map2.shape, order=3, mode='nearest') # bi-cubic/nearest is what Matlab imresize() does by default\n",
    "    # Normalize the two maps to have values between [0,1] and sum up to 1\n",
    "    map1 = normalize(map1, method='range')\n",
    "    map2 = normalize(map2, method='range')\n",
    "    map1 = normalize(map1, method='sum')\n",
    "    map2 = normalize(map2, method='sum')\n",
    "    # Compute histogram intersection\n",
    "    intersection = np.minimum(map1, map2)\n",
    "    return np.sum(intersection)\n",
    "\n",
    "def CC(saliency_map1, saliency_map2):\n",
    "    '''\n",
    "    Pearson's correlation coefficient between two different saliency maps\n",
    "    (CC=-1 inverse linear co-relation for CC=0 for uncorrelated maps, CC=1 for perfect linear correlation).\n",
    "    '''\n",
    "    map1 = np.array(saliency_map1, copy=False)\n",
    "    map2 = np.array(saliency_map2, copy=False)\n",
    "    if map1.shape != map2.shape:\n",
    "        map1 = resize(map1, map2.shape, order=3, mode='nearest') # bi-cubic/nearest is what Matlab imresize() does by default\n",
    "    # Normalize the two maps to have zero mean and unit std\n",
    "    map1 = normalize(map1, method='standard')\n",
    "    map2 = normalize(map2, method='standard')\n",
    "    # Compute correlation coefficient\n",
    "    return np.corrcoef(map1.ravel(), map2.ravel())[0,1]\n",
    "\n",
    "def NSS(saliency_map, fixation_map):\n",
    "    '''\n",
    "    Normalized scanpath saliency of a saliency map, defined as the mean value of normalized (i.e., standardized) saliency map at fixation locations.\n",
    "    You can think of it as a z-score. (Larger value implies better performance.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    saliency_map : real-valued matrix\n",
    "        If the two maps are different in shape, saliency_map will be resized to match fixation_map..\n",
    "    fixation_map : binary matrix\n",
    "        Human fixation map (1 for fixated location, 0 for elsewhere).\n",
    "    Returns\n",
    "    -------\n",
    "    NSS : float, positive\n",
    "    '''\n",
    "    s_map = np.array(saliency_map, copy=False)\n",
    "    f_map = np.array(fixation_map, copy=False) > 0.5\n",
    "    if s_map.shape != f_map.shape:\n",
    "        s_map = resize(s_map, f_map.shape)\n",
    "    # Normalize saliency map to have zero mean and unit std\n",
    "    s_map = normalize(s_map, method='standard')\n",
    "    # Mean saliency value at fixation locations\n",
    "    return np.mean(s_map[f_map])\n",
    "\n",
    "\n",
    "def AUC_Judd(saliency_map, fixation_map, jitter=True):\n",
    "    '''\n",
    "    AUC stands for Area Under ROC Curve.\n",
    "    \n",
    "    True positive (tp) rate correspond to the ratio of saliency map values above threshold\n",
    "    at fixation locations to the total number of fixation locations.\n",
    "    \n",
    "    False positive (fp) rate correspond to the ratio of saliency map values above threshold\n",
    "    at all other locations to the total number of possible other locations (non-fixated image pixels).\n",
    "\n",
    "    AUC=0.5 is chance level.\n",
    "    '''\n",
    "    \n",
    "    saliency_map = np.array(saliency_map, copy=False)\n",
    "    fixation_map = np.array(fixation_map, copy=False) > 0.5\n",
    "    # If there are no fixation to predict, return NaN\n",
    "    if not np.any(fixation_map):\n",
    "        print('no fixation to predict')\n",
    "        return np.nan\n",
    "    # Make the saliency_map the size of the fixation_map\n",
    "    if saliency_map.shape != fixation_map.shape:\n",
    "        saliency_map = resize(saliency_map, fixation_map.shape, order=3, mode='nearest')\n",
    "    # Jitter the saliency map slightly to disrupt ties of the same saliency value\n",
    "    if jitter:\n",
    "        saliency_map += random.rand(*saliency_map.shape) * 1e-7\n",
    "    # Normalize saliency map to have values between [0,1]\n",
    "    saliency_map = normalize(saliency_map, method='range')\n",
    "\n",
    "    S = saliency_map.ravel()\n",
    "    F = fixation_map.ravel()\n",
    "    S_fix = S[F] # Saliency map values at fixation locations\n",
    "    n_fix = len(S_fix)\n",
    "    n_pixels = len(S)\n",
    "    # Calculate AUC\n",
    "    thresholds = sorted(S_fix, reverse=True)\n",
    "    tp = np.zeros(len(thresholds)+2)\n",
    "    fp = np.zeros(len(thresholds)+2)\n",
    "    tp[0] = 0; tp[-1] = 1\n",
    "    fp[0] = 0; fp[-1] = 1\n",
    "    for k, thresh in enumerate(thresholds):\n",
    "        above_th = np.sum(S >= thresh) # Total number of saliency map values above threshold\n",
    "        tp[k+1] = (k + 1) / float(n_fix) # Ratio saliency map values at fixation locations above threshold\n",
    "        fp[k+1] = (above_th - k - 1) / float(n_pixels - n_fix) # Ratio other saliency map values above threshold\n",
    "    return np.trapz(tp, fp) # y, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42526d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eed4002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.export import nb_export\n",
    "nb_export('saliency_verification_tests.ipynb', '/commons/api/') # arg1 : name of notebook, arg2 : location of the export relative to the current directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
